# install.packages('keras')
# install.packages('tensorflow')
library(keras)
install_keras()
install_keras()
y
# install.packages('keras')
# install.packages('tensorflow')
install_keras(tensorflow = 'gpu')
# install.packages('keras')
# install.packages('tensorflow')
install_keras(tensorflow = 'gpu')
# install.packages('keras')
# install.packages('tensorflow')
install_keras(tensorflow = 'gpu')
fasion_mnist = dataset_fashion_mnist(
)
fasion_mnist = dataset_fashion_mnist()
library(keras)
fasion_mnist = dataset_fashion_mnist()
install.packages('keras')
install_keras(tensorflow = 'gpu')
install.packages('tensorflow')
install.packages('keras')
library(keras)
fasion_mnist = dataset_fashion_mnist()
fasion_mnist = dataset_fashion_mnist()
reticulate:::rm_all_reticulate_state()
remotes::install_github("rstudio/reticulate")
install.packages('keras')
install.packages('keras')
# install.packages('keras')
install.packages('tensorflow')
# install.packages('keras')
# install.packages('tensorflow')
install_keras(tensorflow = 'gpu')
fasion_mnist = dataset_fashion_mnist()
library(keras)
fasion_mnist = dataset_fashion_mnist()
y
fasion_mnist = dataset_fashion_mnist()
library(keras)
fasion_mnist = dataset_fashion_mnist()
reticulate:::rm_all_reticulate_state()
install_keras()
y
extra <- "C:/Users/Raf/Anaconda3;C:/Users/Raf/Anaconda3/Library/mingw-w64/bin;C:/Users/Raf/Anaconda3/Library/usr/bin;C:/Users/Raf/Anaconda3/Library/bin;C:/Users/Raf/Anaconda3/Scripts;C:/Users/Raf/Anaconda3/bin;C:/Users/Raf/Anaconda3/condabin;"
Sys.setenv(PATH = paste(extra, Sys.getenv("PATH"), sep = ""))
conda_create("r-tensorflow")
fasion_mnist = dataset_fashion_mnist()
fasion_mnist = dataset_fashion_mnist()
# install.packages('keras')
# install.packages('tensorflow')
install.packages('reticulate')
# install.packages('keras')
# install.packages('tensorflow')
install.packages('reticulate')
install.packages('reticulate')
reticulate::py_discover_config()
install.packages('keras')
install.packages('tensorflow')
install.packages('reticulate')
install.packages('reticulate')
library(keras)
fasion_mnist = dataset_fashion_mnist()
fasion_mnist = dataset_fashion_mnist()
install_keras()
# install_keras()
install_keras(tensorflow = 'gpu')
install_keras(tensorflow = 'gpu')
reticulate::py_discover_config()
keras::is_keras_available()
install_keras(tensorflow = 'gpu')
execstack -c anaconda3/lib/libcrypto.so.1.0.0
remotes::install_github("rstudio/reticulate")
install.packages('keras')
library(keras)
fasion_mnist = dataset_fashion_mnist()
install.packages('keras')
library(keras)
install.packages("keras")
library(keras)
install_keras()
fashion_mnist <- dataset_fashion_mnist()
reticulate::install_miniconda()
fasion_mnist = dataset_fashion_mnist()
reticulate:::rm_all_reticulate_state()
library(keras)
fasion_mnist = dataset_fashion_mnist()
y
library(keras)
fasion_mnist = dataset_fashion_mnist()
library(keras)
library(tensorflow)
install_keras()
# importando librerias
library(keras)
# importando datos
fasion_mnist = dataset_fashion_mnist()
# install_keras()
install_keras(tensorflow = 'gpu')
install_keras(tensorflow = 'gpu')
install_tensorflow()
install_tensorflow()
install_keras()
install_keras()
install_keras(tensorflow = 'gpu')
# importando datos
fasion_mnist = dataset_fashion_mnist()
View(fasion_mnist)
# importando datos
fashion_mnist = dataset_fashion_mnist()
# test-train-split
c(train_images, train_labels) %<-% fashion_mnist$train
c(test_images, test_labels)   %<-% fashion_mnist$test
# Explorando la Estructura de los Datos
dim(train_images)
str(train_images)
# Visualizando la imagen
fobject <- train_images[5,,]
plot(as.raster(fobject, max = 255))
View(fashion_mnist)
# Visualizando la imagen
fobject <- train_images[1,,]
plot(as.raster(fobject, max = 255))
# Visualizando la imagen
fobject <- train_images[10,,]
plot(as.raster(fobject, max = 255))
# Visualizando la imagen
fobject <- train_images[5,,]
plot(as.raster(fobject, max = 255))
class_names = c('T-shirts/top',
'Trouser',
'Pullover',
'Dress',
'Coat',
'Sandal',
'Shirt',
'Sneaker',
'Bag',
'Ankle boot')
class_names[train_labels[5]+1]
# Normalizando [(X-mean)/std.Dev]
train_images <- train_images/255
test_images  <- test_images/255
# Creando una validacion por separacion - usando el sincronizado del hiperparametro
val_indices <- 1:5000
val_images  <- train_images[val_indices,,]
part_train_images <- train_images[-val_indices,,]
val_labels <- train_labels[val_indices]
part_train_labels <- train_labels[-val_indices]
# Acotamiento
model <- keras_model_sequential()
model %>% compile(
optimizer = 'sgd',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy'))
model %>%
layer_flatten(input_shape = c(28,28)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'sgd',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy'))
model %>% fit(part_train_images, part_train_labels, epochs = 30, batch_size=100, validation_data=list(val_images, val_labels))
# Prueba de Rendimiento
score <- model %>% evaluate(test_images, test_labels)
cat('Test loss:', score$loss, '\n')
cat('Test accuracy:', score$acc,'\n')
cat('Test loss:', score$loss, "\n")
score$loss
score
cat('Test loss:', score[loss], "\n")
cat('Test loss:', score$loss, "\n")
cat('Test loss:', score['loss'], "\n")
cat('Test accuracy:', score['acc'], "\n")
cat('Test accuracy:', score['accuracy'], "\n")
# Prediccion en el Test set
predictions <- model %>% predict(test_images)
predictions[1,]
which.max(predictions[1,])
class_names[which.max(predictions[1,])]
plot(as.raster(test_images[1,,], max = 255))
plot(as.raster(test_images[1, , ], max = 255))
plot(as.raster(test_images[1, , ], max = 255))
class_pred <- model %>% predict_classes((test_images))
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]
class_pred <- model %>% predict_classes(test_images)
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]
model
class_pred <- model %>% predict_classes(test_images)
class_pred <- model %>% predict(test_images)
class_pred[1:20]
class_pred <- model %>% predict_classes(test_images)
class_pred <- model %>% predict(test_images) %>% k_argmax()
class_pred[1:20]
install.packages("neuralnet")
# install.packages("neuralnet")
require(neuralnet)
hours    = c(20,1030,20,50,30)
mocktest = c(90,20,20,10,50,80)
Passed   = c(1,0,0,0,1,1)
df = data.frame(Passed,hours,mocktest)
nn = neuralnet(Passed~hours+mocktest, data=df, hidden=c(3,2), act.fct='logistic', linear.output = FALSE)
df = data.frame(hours,mocktest,Passed)
hours    = c(20,10,30,20,50,30)
mocktest = c(90,20,20,10,50,80)
Passed   = c(1,0,0,0,1,1)
df = data.frame(hours,mocktest,Passed)
nn = neuralnet(Passed~hours+mocktest, data=df, hidden=c(3,2), act.fct='logistic', linear.output = FALSE)
plot(nn)
plot(nn)
thours    = c(20,20,30)
tmocktest = c(80,30,20)
test      = data.frame(thours,tmocktest)
Predict = compute(nn, test)
Predict$net.result
prob <- Predict$net.result
pred <- ifelse(prob>0.5, 1, 0)
pred
# importando datos
boston_housing <- dataset_boston_housing()
View(boston_housing)
# test-train split
c(train_data, train_labels) %>% boston_housing$train
# test-train split
c(train_data, BHtrain_labels) %>% boston_housing$train
c(test_data, BHtest_labels) %>% boston_housing$test
c(test_data, BHtest_labels) %<-% boston_housing$test
# test-train split
c(train_data, BHtrain_labels) %<-% boston_housing$train
# Normalizando los datos
train_data <- scale(train_data)
col_means_train <- attr(train_data, 'scaled:center')
col_stddevs_train <- attr(train_data, 'scaled:scale')
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
# Capa de entrada
inputs <- layer_input(shape = dim(train_data)[2])
# Capa de salida
BHpredictions <- inputs %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
# Creando y compilando el modelo
model <- keras_model(inputs = inputs, outputs = BHpredictions)
model %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = list('mean_absolute_error')
)
model <- fit(train_data,BHtrain_labels,epochs=30,batch_size=100)
model %>% fit(train_data,BHtrain_labels,epochs=30,batch_size=100)
BHscore <- model %>% evaluate(test_data, BHtest_labels)
cat('Test loss:', BHscore['loss'], "\n")
cat('Test accuracy:', BHscore['accuracy'], "\n")
cat('Test accuracy:', BHscore['mse'], "\n")
cat('Test absolute error:', BHscore['mean_abolute_error'], '\n')
score
cat('Test absolute error:', BHscore['accuracy'], '\n')
BHscore
cat('Test absolute error:', BHscore['mean_absolute_error'], '\n')
# Capa de entrada
inputs_func <- layer_input(shape = dim(train_data)[2])
library(keras)
# Capa de entrada
inputs_func <- layer_input(shape = dim(train_data)[2])
View(inputs_func)
# install_keras()
install_keras(tensorflow = 'gpu')
# install_keras()
install_tensorflow('gpu')
# install_keras()
install_tensorflow()
# Capa de salida
BHpredictions_func <- inputs_func %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu')
library(tensorflow)
# Capa de salida
BHpredictions_func <- inputs_func %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu')
main_output <- layer_concatenate(c(BHpredictions_func, inputs_func)) %>%
layer_dense(units = 1)
# Crear y compilar el modelo
model_func <- keras_model(inputs = inputs_func, outputs = main_output)
model_func %>% compile(
optimizer = 'rmsprop',
loss      = 'mse',
metrics   = list('mean_absolute_error')
)
summary(model_func())
summary(model_func)
model_func %>% fit(train_data, BHtrain_labels, epochs=30, batch_size=100)
# Prueba de rendimiento
score_func <- model_func %>% evaluate(test_data, BHtest_labels)
cat('Functional model test loss', score_func['loss'], '\n')
cat('Normal model test loss', score['loss'], '\n')
cat('Functional model test Meean abs error', score_func['mean_absolute_error'], '\n')
cat('Normal  model test Meean abs error', score_func['mean_absolute_error'], '\n')
cat('Normal model test Mean abs error', score['mean_absolute_error'], '\n')
score
cat('Normal model test Mean abs error', score['accuracy'], '\n')
# importando datos
boston_housing <- dataset_boston_housing()
# test-train split
c(train_data, BHtrain_labels) %<-% boston_housing$train
c(test_data, BHtest_labels) %<-% boston_housing$test
# Normalizando los datos
train_data <- scale(train_data)
col_means_train   <- attr(train_data, 'scaled:center')
col_stddevs_train <- attr(train_data, 'scaled:scale')
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
# Capa de entrada
inputs <- layer_input(shape = dim(train_data)[2])
# Capa de salida
BHpredictions <- inputs %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
# Creando y compilando el modelo
model <- keras_model(inputs = inputs, outputs = BHpredictions)
model %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = list('mean_absolute_error')
)
model %>% fit(train_data,BHtrain_labels,epochs=30,batch_size=100)
BHscore <- model %>% evaluate(test_data, BHtest_labels)
cat('Test loss:', BHscore['loss'], "\n")
cat('Test absolute error:', BHscore['mean_absolute_error'], '\n')
# Capa de entrada
inputs_func <- layer_input(shape = dim(train_data)[2])
# Capa de salida
BHpredictions_func <- inputs_func %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu')
main_output <- layer_concatenate(c(BHpredictions_func, inputs_func)) %>%
layer_dense(units = 1)
# Crear y compilar el modelo
model_func <- keras_model(inputs = inputs_func, outputs = main_output)
model_func %>% compile(
optimizer = 'rmsprop',
loss      = 'mse',
metrics   = list('mean_absolute_error')
)
summary(model_func)
model_func %>% fit(train_data, BHtrain_labels, epochs=30, batch_size=100)
# Prueba de rendimiento
score_func <- model_func %>% evaluate(test_data, BHtest_labels)
cat('Functional model test loss', score_func['loss'], '\n')
cat('Normal model test loss', BHscore['loss'], '\n')
cat('Functional model test Mean abs error', score_func['mean_absolute_error'], '\n')
cat('Normal model test Mean abs error', BHscore['mean_absolute_error'], '\n')
# Guardando-Restaurando modelo y llamandolos de nuevo
model_func %>% save_model_hdf5('my_model.h5')
new_model  %>% load_model_hdf5('my_model.h5')
new_model  %>% load_model_hdf5('my_model.h5')
model_new  %>% load_model_hdf5('my_model.h5')
model_new  <-  load_model_hdf5('my_model.h5')
model_func %>% summary()
model_new  %>% summary()
# Guardando puntos de carga de epochs
checkpoint_dir <- 'checkpoints'
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath <- file.path(checkpoint_dir, 'Epoch={epoch:02d}.hdf5')
cp_callback <- callback_model_checkpoint(filepath = filepath)
rm(model_func())
k_clear_session()
rm(model_func)
k_clear_session()
model_callback <- keras_model(inputs =  inputs_func, outputs = main_output)
model_callback %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = list('man_absolute_error')
)
model_callback %>% fit(train_data, BHtrain_labels, epochs=30,
callbacks = list(cp_callback))
model_callback %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = list('mean_absolute_error')
)
model_callback %>% fit(train_data, BHtrain_labels, epochs=30,
callbacks = list(cp_callback))
list.files(checkpoint_dir)
tenth_model <- load_model_hdf5(file.path(checkpoint_dir, 'Epoch=10.hdf5'))
summary(tenth_model())
summary(tenth_model)
rm(model_callback)
k_clear_session()
# Guardando solo el mejor modelo
callback_best <- callback_model_checkpoint(filepath = 'best_model.h5', monitor = 'val_loss',
save_best_only =TRUE)
model_cb_best %>% keras_model(inputs = inputs_func, outputs = main_output)
model_cb_best <- keras_model(inputs = inputs_func, outputs = main_output)
model_cb_best %>% compile(
optimizer = 'rmsprop',
loss      = 'mse',
metrics   = list('mean_absolute_error')
)
model_cb_best %>% fit(train_data, BHtrain_labels, epochs=30,
validation_data=list(test_data,BHtest_labels),
callbacks = list(callback_best))
best_model <- load_model_hdf5('best_model.h5')
# Detener los porcesos cuando se halla el mejor modelo
callback_list <- list(
callback_early_stopping(monitor =  'val_loss', patience = 3),
callback_model_checkpoint(filepath = 'best_model_early_stopping.h5', monitor = 'val_loss', save_best_only = TRUE)
)
rm(model_cb_best)
k_clear_session()
model_cb_early <- keras_model(inputs = inputs_func, outputs = main_output)
model_cb_early %>% compile(
optimizer = 'rmsprop',
loss      = 'mse',
metrics   = list('mean_absolute_error')
)
model_cb_early %>% fit(train_data, BHtrain_labels, epochs=100,
validation_data=list(test_data,BHtest_labels),
callbacks = list(callback_list))
