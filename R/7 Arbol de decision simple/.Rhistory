df <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_regression.csv", header = TRUE)
View(df)
rm(df)
# Pre-prosesamiento de datos
summary(movie)
#  Importando Datos
movie <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_regression.csv", header = TRUE)
# Pre-prosesamiento de datos
summary(movie)
movie$Time_taken[is.na(movie$Time_taken)] <- mean(movie$Time_taken,na.rm = TRUE)
summary(movie$Time_taken)
View(movie)
# Test-Train split
library(caTools)
set.seed(0)
split = sample.split(movie,SplitRatio =  0.8)
train =subset(movie, plit == TRUE)
test =subset(movie,split == FALSE)
train =subset(movie, plit == TRUE)
train = subset(movie, split == TRUE)
summary(test)
test
#  Paquetes requeridos
library(rpart)
#  Paquetes requeridos
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
# Correr a regresion de modelo de arbol en train set
regtree <- rpart(formula = Collection~., data = train, control = rpart.control(maxdepth = 3))
# Grafica del Arbol  de desicion
rpart.plot(redtree,box.palette = "RdBu",digits = 3)
# Grafica del Arbol  de desicion
rpart.plot(regtree,box.palette = "RdBu",digits = 3)
# Prediccion de valores en cualquier punto
test$pred <- predict(regtree, test, type = "vector")
MSE2 <- mean((test$pred - test$Collection)^2)
# Grafica del Arbol  de desicion
rpart.plot(regtree,box.palette = "RdBu",digits = 3)
#  Paquetes requeridos
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
# Correr a regresion de modelo de arbol en train set
regtree <- rpart(formula = Collection~., data = train, control = rpart.control(maxdepth = 3))
# Grafica del Arbol  de desicion
rpart.plot(regtree,box.palette = "RdBu",digits = 3)
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
printcp(fulltree)
plotcp(regtree)
mincp <- regtree$cptable(which.min(regtree$cptable[,"xerror"]),"CP")
mincp <- regtree$cptable(which.min(regtree$cptable[,"xerror"]),"CP")
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
printcp(fulltree)
plotcp(regtree)
mincp <- regtree$cptable(which.min(regtree$cptable[,"xerror"]),"CP")
mincp <- regtree$cptable[which.min(regtree$cptable[,"xerror"]),"CP"]
prunedtree <- prune(regtree, cp = mincp)
rpart.plot(prunedtree, box.palette = "RdBu", digits = 3)
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
printcp(fulltree)
plotcp(regtree)
mincp <- regtree$cptable[which.min(regtree$cptable[,"xerror"]),"CP"]
prunedtree <- prune(regtree, cp = mincp)
rpart.plot(prunedtree, box.palette = "RdBu", digits = 3)
test$fulltree <- predict(fulltree, test, type = "vector")
MSE2 <- mean((test$fulltree - test$Collection)^2)
test$pruned <- predict(prunedtree, test, type = "vector")
MSE2pruned <- mean((test$pruned - test$Collection)^2)
accuracy_postprun <- mean(test$pred == test$left)
df  <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_regression.csv", header = TRUE)
View(df)
rm(df)
df  <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_classification.csv", header = TRUE)
View(df)
# Pre-procesamiento de Datos
summary(df)
# Pre-procesamiento de Datos
summary(df)
df$Time_taken[is.na(df$Time_taken)] <- mean(df$Time_taken, na.rm = TRUE)
# Test-Train Split
library(caTools)
set.seed(0)
split = sample.split(movie,SplitRatio = 0.8)
trainc = subset(df, split == TRUE)
testc = subset(df,split == FALSE)
library(rpart, rpart.plot)
library(rpart.plot)
calsstree <- rpart(formula = Start_Tech_Oscar~., data = trainc, method = 'class', control = rpart.control(maxdepth = 3))
rm(calsstree)
classtree <- rpart(formula = Start_Tech_Oscar~., data = trainc, method = 'class', control = rpart.control(maxdepth = 3))
# Grafica del Arbol de Decision
rpart.plot(classtree, box.palette = "RdBu", digits = 3)
# Prediccion de valore en cualquier punto
test$pred <- predict(classtree, testc, type = "class")
table(testc$Start_Tech_Oscar,testc$pred)
# Prediccion de valore en cualquier punto
test$pred <- predict(classtree, testc, type = "class")
table(testc$Start_Tech_Oscar,testc$pred)
rm(testc$pred)
rm(test$pred)
rm(test)
# Prediccion de valore en cualquier punto
testc$pred <- predict(classtree, testc, type = "class")
table(testc$Start_Tech_Oscar,testc$pred)
63/112
# Bagging
install.packages("randomForest")
library(randomForest)
bagging = randomForest(Collection~., data = train, mtry=17)
test$bagging <- predict(bagging, test)
MSE2bagging <- mean((test$bagging - test$Collection)^2)
bagging = randomForest(Collection~., data = train, mtry=17, importance = TRUE)
# Test-Train split
library(caTools)
set.seed(0)
split = sample.split(movie, SplitRatio =  0.8)
train = subset(movie, split == TRUE)
test = subset(movie, split == FALSE)
#  Paquetes requeridos
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
# Correr a regresion de modelo de arbol en train set
regtree <- rpart(formula = Collection~., data = train, control = rpart.control(maxdepth = 3))
# Grafica del Arbol  de desicion
rpart.plot(regtree,box.palette = "RdBu",digits = 3)
# Prediccion de valores en cualquier punto
test$pred <- predict(regtree, test, type = "vector")
MSE2 <- mean((test$pred - test$Collection)^2)
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
printcp(fulltree)
plotcp(regtree)
mincp <- regtree$cptable[which.min(regtree$cptable[,"xerror"]),"CP"]
prunedtree <- prune(regtree, cp = mincp)
rpart.plot(prunedtree, box.palette = "RdBu", digits = 3)
test$fulltree <- predict(fulltree, test, type = "vector")
MSE2 <- mean((test$fulltree - test$Collection)^2)
test$pruned <- predict(prunedtree, test, type = "vector")
MSE2pruned <- mean((test$pruned - test$Collection)^2)
accuracy_postprun <- mean(test$pred == test$left)
data.frame(base_accuracy, accuracy_preprun, accuracy_postprun)
# Arbol de Clasificacion
library(datasets)
df  <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_classification.csv", header = TRUE)
View(df)
# Pre-procesamiento de Datos
summary(df)
df$Time_taken[is.na(df$Time_taken)] <- mean(df$Time_taken, na.rm = TRUE)
# Test-Train Split
library(caTools)
set.seed(0)
split = sample.split(movie,SplitRatio = 0.8)
trainc = subset(df, split == TRUE)
testc = subset(df,split == FALSE)
# Correr el Modelo de Arbol de Clasificacion en train set
library(rpart)
library(rpart.plot)
classtree <- rpart(formula = Start_Tech_Oscar~., data = trainc, method = 'class', control = rpart.control(maxdepth = 3))
# Grafica del Arbol de Decision
rpart.plot(classtree, box.palette = "RdBu", digits = 3)
# Prediccion de valore en cualquier punto
testc$pred <- predict(classtree, testc, type = "class")
table(testc$Start_Tech_Oscar,testc$pred)
# Bagging
# install.packages("randomForest")
library(randomForest)
set.seed(0)
bagging = randomForest(Collection~., data = train, mtry=17)
test$bagging <- predict(bagging, test)
MSE2bagging <- mean((test$bagging - test$Collection)^2)
bagging = randomForest(Collection~., data = train, mtry=17, importance = TRUE)
which(is.na)
bagging = randomForest(formula = Collection~., data = train, mtry=17)
View(bagging)
test$bagging <- predict(bagging, test)
test$bagging <- predict(bagging, test)
View(test)
View(test)
MSE2bagging <- mean((test$bagging - test$Collection)^2)
# Random Forest
randomfor <- randomForest(Collection~., data = train, ntree=500)
# Random Forest
randomfor <- randomForest(Collection~., data = train, ntree=500)
# salida de prediccion
test$random <- predict(randomfor,test)
MSE2random <- mean((test$random - test$Collection)^2)
# Gradient Boosting Model GBM
install.packages("gbm")
# Gradient Boosting Model GBM
# install.packages("gbm")
library(gbm)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
set.seed(0)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
# Gradient Boosting Model GBM
# install.packages("gbm")
library(gbm)
set.seed(0)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
boosting = gbm(Collection~., data = train, distribution = gaussian, n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
#  Creacion variables Dummies
library(dummies)
df <- dummy.data.frame(df)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
View(train)
train <- dummy.data.frame(train)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
# distribution = 'Gaussian' for regression and 'Bernoulli' for classification
test$boost = predict(boosting, test, n.trees = 5000)
MSE2boost <- mean((test$boost - test$Collection)^2)
# distribution = 'Gaussian' for regression and 'Bernoulli' for classification
test$boost = predict(boosting, test, n.trees = 5000)
View(boosting)
# distribution = 'Gaussian' for regression and 'Bernoulli' for classification
test$boost <- predict(boosting, test, n.trees = 5000)
view(test)
View(test)
# distribution = 'Gaussian' for regression and 'Bernoulli' for classification
test <- dummy.data.frame(test)
test$boost = predict(boosting, test, n.trees = 5000)
MSE2boost <- mean((test$boost - test$Collection)^2)
# Adaboost
install.packages("adabag")
# Adaboost
# install.packages("adabag")
library(adabag)
# Adaboost
# install.packages("adabag")
library(adabag)
# Adaboost
# install.packages("adabag")
library(adabag);
# Adaboost
# install.packages("adabag")
library(adabag)
adaboost <- boosting(Start_Tech_Oscar1~.-Start_Tech_Oscar, data = trainc, boos = TRUE)
trainc$Start_Tech_Oscar1 <- as.factor(trainc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar1~.-Start_Tech_Oscar, data = trainc, boos = TRUE)
predada <- predict(adaboost, testc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar1~.-Start_Tech_Oscar, data = trainc, boos = TRUE)
predada <- predict(adaboost, testc$Start_Tech_Oscar)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
t1 <- adaboost$trees[[1]]
plot(t1)
text(t1, pretty=100)
plot(t1)
text(t1, pretty=100)
text(t1, pretty=100)
plot(t1)
text(t1, pretty=100)
trainc$Start_Tech_Oscar <- as.factor(trainc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar1~.-Start_Tech_Oscar, data = trainc, boos = TRUE)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
t1 <- adaboost$trees[[1]]
plot(t1)
text(t1, pretty=100)
trainc$Start_Tech_Oscar <- as.factor(trainc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar1~.-Start_Tech_Oscar, data = trainc, boos = TRUE)
View(trainc)
adaboost <- boosting(Start_Tech_Oscar, data = trainc, boos = TRUE)
adaboost <- boosting(Start_Tech_Oscar~., data = trainc, boos = TRUE)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
t1 <- adaboost$trees[[1]]
plot(t1)
text(t1, pretty=100)
View(trainc)
View(trainc)
trainc[,-20]
View(trainc)
rm(trainc[,-20])
trainc[,-20]
table(predada$class,testc$Start_Tech_Oscar)
70/113
adaboost <- boosting(Start_Tech_Oscar~., data = trainc, boos = TRUE,mfinal=1000)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
table(predada$class,testc$Start_Tech_Oscar)
69/113
t1 <- adaboost$trees[[1]]
plot(t1)
text(t1, pretty=100)
#  Importando Datos
movie <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_regression.csv", header = TRUE)
# Pre-prosesamiento de datos
summary(movie)
movie$Time_taken[is.na(movie$Time_taken)] <- mean(movie$Time_taken,na.rm = TRUE)
summary(movie$Time_taken)
movie$Time_taken[is.na(movie$Time_taken)] <- mean(movie$Time_taken,na.rm = TRUE)
summary(movie$Time_taken)
# Test-Train split
library(caTools)
set.seed(0)
split = sample.split(movie, SplitRatio =  0.8)
library(dummies)
movie <- dummy.data.frame(movie)
train = subset(movie, split == TRUE)
test = subset(movie, split == FALSE)
#  Paquetes requeridos
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
# Correr a regresion de modelo de arbol en train set
regtree <- rpart(formula = Collection~., data = train, control = rpart.control(maxdepth = 3))
# Grafica del Arbol  de desicion
rpart.plot(regtree,box.palette = "RdBu",digits = 3)
# Prediccion de valores en cualquier punto
test$pred <- predict(regtree, test, type = "vector")
MSE2 <- mean((test$pred - test$Collection)^2)
# Podado del Arbol
fulltree <- rpart(formula = Collection~., data = train, control = rpart.control(cp=0))
rpart.plot(fulltree,box.palette = "RdBu", digits = 3)
printcp(fulltree)
plotcp(regtree)
mincp <- regtree$cptable[which.min(regtree$cptable[,"xerror"]),"CP"]
prunedtree <- prune(regtree, cp = mincp)
rpart.plot(prunedtree, box.palette = "RdBu", digits = 3)
test$fulltree <- predict(fulltree, test, type = "vector")
MSE2 <- mean((test$fulltree - test$Collection)^2)
test$pruned <- predict(prunedtree, test, type = "vector")
MSE2pruned <- mean((test$pruned - test$Collection)^2)
accuracy_postprun <- mean(test$pred == test$left)
data.frame(base_accuracy, accuracy_preprun, accuracy_postprun)
# Arbol de Clasificacion
library(datasets)
df  <- read.csv("C:/Users/Raf/Documents/R/7 Arbol de decision simple/Movie_classification.csv", header = TRUE)
View(df)
# Pre-procesamiento de Datos
summary(df)
df$Time_taken[is.na(df$Time_taken)] <- mean(df$Time_taken, na.rm = TRUE)
# Test-Train Split
library(caTools)
set.seed(0)
split = sample.split(movie,SplitRatio = 0.8)
trainc = subset(df, split == TRUE)
testc = subset(df,split == FALSE)
# Correr el Modelo de Arbol de Clasificacion en train set
library(rpart)
library(rpart.plot)
classtree <- rpart(formula = Start_Tech_Oscar~., data = trainc, method = 'class', control = rpart.control(maxdepth = 3))
# Grafica del Arbol de Decision
rpart.plot(classtree, box.palette = "RdBu", digits = 3)
# Prediccion de valore en cualquier punto
testc$pred <- predict(classtree, testc, type = "class")
table(testc$Start_Tech_Oscar,testc$pred)
73/112
# Bagging
# install.packages("randomForest")
library(randomForest)
set.seed(0)
bagging = randomForest(formula = Collection~., data = train, mtry=17)
test$bagging <- predict(bagging, test)
MSE2bagging <- mean((test$bagging - test$Collection)^2)
# Random Forest
# library(randomForest)
randomfor <- randomForest(Collection~., data = train, ntree=500)
# salida de prediccion
test$random <- predict(randomfor,test)
MSE2random <- mean((test$random - test$Collection)^2)
# Gradient Boosting Model GBM
# install.packages("gbm")
library(gbm)
set.seed(0)
#  Creacion variables Dummies
# library(dummies)
# train <- dummy.data.frame(train)
boosting = gbm(Collection~., data = train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
# distribution = 'Gaussian' for regression and 'Bernoulli' for classification
# test <- dummy.data.frame(test)
test$boost = predict(boosting, test, n.trees = 5000)
MSE2boost <- mean((test$boost - test$Collection)^2)
# Adaboost
# install.packages("adabag")
library(adabag)
trainc$Start_Tech_Oscar <- as.factor(trainc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar~., data = trainc, boos = TRUE,mfinal=1000)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar~., data = trainc, boos = TRUE,mfinal=1000)
View(boosting)
View(boosting)
predada <- predict(adaboost, testc)
trainc$Start_Tech_Oscar <- as.factor(trainc$Start_Tech_Oscar)
adaboost <- boosting(Start_Tech_Oscar~., data = trainc, boos = TRUE,mfinal=1000)
View(trainc)
predada <- predict(adaboost, testc)
table(predada$class,testc$Start_Tech_Oscar)
69/113
t1 <- adaboost$trees[[1]]
plot(t1)
text(t1, pretty=100)
# XGboost
install.packages("xgboost")
library(xgboost)
trainY = trainc$Start_Tech_Oscar == "1"
trainX = model.matrix(Start_Tech_Oscar ~ . -1, data = trainc)
View(trainX)
View(trainX)
train[,-12]
trainX <- trainX[,-12]
testY = testc$Start_Tech_Oscar == "1"
view(testY)
testX = model.matrix(Start_Tech_Oscar ~ . -1, data = testc)
View(testX)
testX <- testX[,-12]
# borrar variables adicionales
Xmatrix <- xgb.DMatrix(data = trainX, label= trainY)
Xmatrix_t <- xgb.DMatrix(data = testX, label= testY)
Xgboosting <- xgboost(data = Xmatrix, #los datos
nround = 50, #numero max. de iteraciones boosting
objetive = "multi:softmax",eta=0.3, num_class = 2, max_depth = 100)
xpred <- predict(Xboosting, Xmatrix_t)
xpred <- predict(Xgboosting, Xmatrix_t)
table(testY, xpred)
xgpred <- predict(Xgboosting, Xmatrix_t)
Xgboosting <- xgboost(data = Xmatrix, #los datos
nround = 50, #numero max. de iteraciones boosting
objetive = "multi:softmax",eta=0.3, num_class = 2, max_depth = 100)
xgpred <- predict(Xgboosting, Xmatrix_t)
View(Xgboosting)
# borrar variables adicionales
testX <- testX[,-21:-23]
Xmatrix <- xgb.DMatrix(data = trainX, label= trainY)
Xmatrix_t <- xgb.DMatrix(data = testX, label= testY)
Xgboosting <- xgboost(data = Xmatrix, #los datos
nround = 50, #numero max. de iteraciones boosting
objetive = "multi:softmax",eta=0.3, num_class = 2, max_depth = 100)
xgpred <- predict(Xgboosting, Xmatrix_t)
table(testY, xgpred)
81/115
Xgboosting <- xgboost(data = Xmatrix, #los datos
nround = 50, #numero max. de iteraciones boosting
objetive = "multi:softmax",eta=0.3, num_class = 2, max_depth = 10)
xgpred <- predict(Xgboosting, Xmatrix_t)
table(testY, xgpred)
74/115
81/115
